import streamlit as st
import re
import docx2txt
import joblib
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer

# Load trained model and vectorizers
model = joblib.load("decision_tree_resume_model.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")
label_encoder = joblib.load("label_encoder.pkl")

# ----------------- Resume Parsing Functions ---------------- #

def extract_text_from_pdf(file):
    pdf = PdfReader(file)
    text = ""
    for page in pdf.pages:
        text += page.extract_text() + "\n"
    return text

def extract_text_from_docx(file):
    return docx2txt.process(file)

def clean_text(text):
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\S+@\S+', '', text)  # remove emails
    text = re.sub(r'http\S+', '', text)  # remove links
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return text.lower()

def extract_email(text):
    match = re.search(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', text)
    return match.group(0) if match else "Not found"

def extract_name(text):
    # Common invalid headers to skip
    invalid_headers = {'professional summary', 'curriculum vitae', 'resume', 'profile', 'about me'}
    
    # Try: "Name: John Doe"
    name_match = re.search(r'(?:name\s*[:\-]\s*)([A-Z][a-z]+(?:\s[A-Z][a-z]+)+)', text, re.IGNORECASE)
    if name_match:
        name = name_match.group(1).strip()
        if name.lower() not in invalid_headers:
            return name

    # Try: first 10 lines for proper name format
    lines = text.strip().split('\n')
    for line in lines[:10]:
        line_clean = line.strip()
        line_lower = line_clean.lower()

        if line_lower in invalid_headers or len(line_clean.split()) > 6:
            continue

        # Match: John Doe, John A. Smith, etc.
        if re.match(r'^([A-Z][a-z]+\s){1,3}[A-Z][a-z]+\.?$', line_clean):
            return line_clean

        # Match all-uppercase: JOHN DOE â†’ John Doe
        if re.match(r'^([A-Z]{2,}\s?){2,3}$', line_clean):
            return line_clean.title()

    # Fallback: return first capitalized line with 2â€“3 titlecase words
    for line in lines[:10]:
        words = line.strip().split()
        capitalized = [w for w in words if w.istitle()]
        if 2 <= len(capitalized) <= 4:
            return " ".join(capitalized)

    # Last resort: return first line
    return lines[0].strip().title()




def extract_skills(text):
    skill_keywords = ['python', 'sql', 'excel', 'tableau', 'powerbi', 'oracle', 'pl/sql',
                      'java', 'react', 'html', 'css', 'javascript', 'c++', 'pandas', 'numpy',
                      'scikit-learn', 'tensorflow', 'keras','microsoft sql server','my sql','ms_excel',
                     'recruitment', 'talent acquisition', 'human resources', 'onboarding', 'payroll',
                     'data analysis','react', 'redux', 'jsx', 'javascript'
                     html', 'css', 'react', 'angular', 'node.js', 'express', 'flutter', 'django',

        
                    'sql', 'mysql', 'postgresql', 'mongodb', 'oracle', 'pl/sql',
                    'pandas', 'numpy', 'scikit-learn', 'tensorflow', 'keras',
                     'machine learning', 'deep learning', 'nlp',

        
                     'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins', 'git',

        
                     'power bi', 'tableau', 'excel', 'matplotlib', 'seaborn', 'looker',

        
                       'sap', 'salesforce', 'jira', 'crm', 'erp', 'peoplesoft']
                       
text = text.lower()
    found_skills = []

    for skill in skill_keywords:
        pattern = r'\b' + re.escape(skill.lower()) + r'\b'
        if re.search(pattern, text):
            found_skills.append(skill)

    return list(set(found_skills)) if found_skills else ["Not found"]
def extract_experience(text):
    lines = text.split('\n')
    exp_lines = [line for line in lines if 'experience' in line.lower()]
    return exp_lines[:3] if exp_lines else ["Not found"]

def extract_education(text):
    edu_keywords = ['education', 'b.tech', 'm.tech', 'bachelor', 'master', 'degree', 'mba', 'b.sc', 'mca', 'bca']
    lines = text.split('\n')
    edu_lines = [line for line in lines if any(keyword in line.lower() for keyword in edu_keywords)]
    return edu_lines[:3] if edu_lines else ["Not found"]

# ----------------- Streamlit App ---------------- #

st.set_page_config(page_title="Resume Job Role Predictor", layout="centered")
st.title("ğŸ“„ Resume Job Role Predictor")
st.write("Upload a resume file (.pdf or .docx) to extract details and predict the job role.")

uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx"])

if uploaded_file:
    if uploaded_file.name.endswith(".pdf"):
        raw_text = extract_text_from_pdf(uploaded_file)
    else:
        raw_text = extract_text_from_docx(uploaded_file)

    name = extract_name(raw_text)
    email = extract_email(raw_text)
    skills = extract_skills(raw_text)
    experience = extract_experience(raw_text)
    education = extract_education(raw_text)

    cleaned = clean_text(raw_text)
    vector = vectorizer.transform([cleaned])
    pred_label = model.predict(vector)[0]
    job_role = label_encoder.inverse_transform([pred_label])[0]

    # ----------------- Display Output ---------------- #
    st.markdown(f"ğŸ‘¤ **Name:** {name}")
    st.markdown(f"ğŸ“§ **Email:** {email}")

    st.markdown("ğŸ›  **Skills:**")
    if skills:
        st.markdown(f"- {', '.join(skills[:4])}")
        st.markdown(f"- {', '.join(skills[4:8])}" if len(skills) > 4 else "")
    else:
        st.write("Not found")

    st.markdown("ğŸ’¼ **Experience:**")
    for line in experience:
        st.markdown(f"- {line.strip()}")

    st.markdown("ğŸ“ **Education:**")
    for line in education:
        st.markdown(f"- {line.strip()}")

    st.success(f"ğŸ§‘â€ğŸ’¼ **Predicted Job Role:** {job_role}")





